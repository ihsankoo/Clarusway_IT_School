{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ckFgXU36QC7I"
   },
   "source": [
    "## Install and Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "juwH-nLwis2a",
    "outputId": "42a1d8ce-ba8e-47c6-ad7e-17fdab2f4dbb"
   },
   "outputs": [],
   "source": [
    "#!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "gwt-asmVfWy1"
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "okmdKmORlmRU",
    "outputId": "77f26116-b18f-48e8-f922-605ef8a63786"
   },
   "outputs": [],
   "source": [
    "#!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "VispLAdSl95W"
   },
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "JVI3B3kcQC7L",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# natural language toolkit\n",
    "#!pip install nltk contractions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rKnwZuyRQC7N"
   },
   "source": [
    "🔑    :     https://www.nltk.org/api/nltk.tokenize.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "stqZ0iNlQC7N"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import contractions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yIVZNeB3QC7O"
   },
   "source": [
    "### Notebook settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "3JPgI4coQC7O"
   },
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', None)\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = 'all'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample Text to process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_text= \"\"\"This is pretty cool. A good quality candy might cost $3.88 in New York. \n",
    "                But I don't think we will buy it. Mr.Biden said $1,000,000. 2 cars.\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lower casing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"this is pretty cool. a good quality candy might cost $3.88 in new york. \\n                but i don't think we will buy it. mr.biden said $1,000,000. 2 cars.\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_text = sample_text.lower()\n",
    "sample_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZrlwxkdFQC7P"
   },
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "pHpS0QoqQC7Q"
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize, wordpunct_tokenize, word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['BlanklineTokenizer',\n",
       " 'LegalitySyllableTokenizer',\n",
       " 'LineTokenizer',\n",
       " 'MWETokenizer',\n",
       " 'NLTKWordTokenizer',\n",
       " 'PunktSentenceTokenizer',\n",
       " 'RegexpTokenizer',\n",
       " 'ReppTokenizer',\n",
       " 'SExprTokenizer',\n",
       " 'SpaceTokenizer',\n",
       " 'StanfordSegmenter',\n",
       " 'SyllableTokenizer',\n",
       " 'TabTokenizer',\n",
       " 'TextTilingTokenizer',\n",
       " 'ToktokTokenizer',\n",
       " 'TreebankWordTokenizer',\n",
       " 'TweetTokenizer',\n",
       " 'WhitespaceTokenizer',\n",
       " 'WordPunctTokenizer',\n",
       " '__builtins__',\n",
       " '__cached__',\n",
       " '__doc__',\n",
       " '__file__',\n",
       " '__loader__',\n",
       " '__name__',\n",
       " '__package__',\n",
       " '__path__',\n",
       " '__spec__',\n",
       " '_treebank_word_tokenizer',\n",
       " 'api',\n",
       " 'blankline_tokenize',\n",
       " 'casual',\n",
       " 'casual_tokenize',\n",
       " 'destructive',\n",
       " 'legality_principle',\n",
       " 'line_tokenize',\n",
       " 'load',\n",
       " 'mwe',\n",
       " 'punkt',\n",
       " 're',\n",
       " 'regexp',\n",
       " 'regexp_span_tokenize',\n",
       " 'regexp_tokenize',\n",
       " 'repp',\n",
       " 'sent_tokenize',\n",
       " 'sexpr',\n",
       " 'sexpr_tokenize',\n",
       " 'simple',\n",
       " 'sonority_sequencing',\n",
       " 'stanford_segmenter',\n",
       " 'string_span_tokenize',\n",
       " 'texttiling',\n",
       " 'toktok',\n",
       " 'treebank',\n",
       " 'util',\n",
       " 'word_tokenize',\n",
       " 'wordpunct_tokenize']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(nltk.tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.word_tokenize??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BwtgntGxQC7Q"
   },
   "source": [
    "#### Sentence Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "ASCpxs_WQQHn"
   },
   "outputs": [],
   "source": [
    "# To use tokenziers\n",
    "#nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this is pretty cool. a good quality candy might cost $3.88 in new york. \n",
      "                but i don't think we will buy it. mr.biden said $1,000,000. 2 cars.\n"
     ]
    }
   ],
   "source": [
    "print(sample_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "Brw2U9CXQC7Q"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['this is pretty cool.',\n",
       " 'a good quality candy might cost $3.88 in new york.',\n",
       " \"but i don't think we will buy it.\",\n",
       " 'mr.biden said $1,000,000.',\n",
       " '2 cars.']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_tokens = sent_tokenize(sample_text)\n",
    "sentence_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3zLpDtxgQC7R"
   },
   "source": [
    "#### WordPunct Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "nks9ILs4QC7R"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['this', 'is', 'pretty', 'cool', '.', 'a', 'good', 'quality', 'candy', 'might', 'cost', '$', '3', '.', '88', 'in', 'new', 'york', '.', 'but', 'i', 'don', \"'\", 't', 'think', 'we', 'will', 'buy', 'it', '.', 'mr', '.', 'biden', 'said', '$', '1', ',', '000', ',', '000', '.', '2', 'cars', '.']\n"
     ]
    }
   ],
   "source": [
    "wordpunc_tokens = wordpunct_tokenize(sample_text)\n",
    "print(wordpunc_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DE3Vs9oTQC7R"
   },
   "source": [
    "#### Word Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "yjJ95R43QC7R"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['this', 'is', 'pretty', 'cool', '.', 'a', 'good', 'quality', 'candy', 'might', 'cost', '$', '3.88', 'in', 'new', 'york', '.', 'but', 'i', 'do', \"n't\", 'think', 'we', 'will', 'buy', 'it', '.', 'mr.biden', 'said', '$', '1,000,000', '.', '2', 'cars', '.']\n"
     ]
    }
   ],
   "source": [
    "word_tokens = word_tokenize(sample_text)\n",
    "print(word_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KqTWZsLdUEQH"
   },
   "source": [
    "#### Tweet Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "WL8YPohQUBQ-"
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import TweetTokenizer\n",
    "tk = TweetTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "oaCpxu7lSZWG"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\"', 'Home', 'home', 'sweet', 'home', '\"']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a string input\n",
    "tweet1 = '&quot;Home home sweet home&quot;' #--> \"Home home sweet home\"\n",
    "result = tk.tokenize(tweet1)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "jzAb0xwAocct"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['&', 'quot', ';', 'Home', 'home', 'sweet', 'home', '&', 'quot', ';']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokenize(tweet1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'really', 'dislike', 'him', '.', '#VivaUSA', '@someone', 'I', 'really', 'enjoy', 'reading', 'a', 'book', '.']\n"
     ]
    }
   ],
   "source": [
    "tweet2 = 'I really dislike him. #VivaUSA @someone I really enjoy reading a book.'\n",
    "print(tk.tokenize(tweet2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'really', 'dislike', 'him', '.', '#', 'VivaUSA', '@', 'someone', 'I', 'really', 'enjoy', 'reading', 'a', 'book', '.']\n"
     ]
    }
   ],
   "source": [
    "print(word_tokenize(tweet2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "plKtrYtIQC7S"
   },
   "source": [
    "## Removing Punctuation and Numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['this', 'is', 'pretty', 'cool', '.', 'a', 'good', 'quality', 'candy', 'might', 'cost', '$', '3.88', 'in', 'new', 'york', '.', 'but', 'i', 'do', \"n't\", 'think', 'we', 'will', 'buy', 'it', '.', 'mr.biden', 'said', '$', '1,000,000', '.', '2', 'cars', '.']\n"
     ]
    }
   ],
   "source": [
    "print(word_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "TOSGvA72QC7S"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['this', 'is', 'pretty', 'cool', 'a', 'good', 'quality', 'candy', 'might', 'cost', 'in', 'new', 'york', 'but', 'i', 'do', 'think', 'we', 'will', 'buy', 'it', 'said', 'cars']\n"
     ]
    }
   ],
   "source": [
    "tokens_without_punc = [w for w in word_tokens if w.isalpha()] # .isalnum() for number and object # we are losing mr.biden\n",
    "print(tokens_without_punc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bqACGNxcQC7S"
   },
   "source": [
    "## Removing Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "J62SbXXRQC7S",
    "outputId": "48b395e7-3fdd-4536-98bc-d92f79642869"
   },
   "outputs": [],
   "source": [
    "#nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "veA2ccj4QC7T"
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "zg0P97lTQC7T"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "stop_words  = stopwords.words('english')\n",
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "NNNf1NVCQC7T"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "179"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stop_words.__len__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "179"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "3DEv2JPfQC7U"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['acaba', 'ama', 'aslında', 'az', 'bazı', 'belki', 'biri', 'birkaç', 'birşey', 'biz', 'bu', 'çok', 'çünkü', 'da', 'daha', 'de', 'defa', 'diye', 'eğer', 'en', 'gibi', 'hem', 'hep', 'hepsi', 'her', 'hiç', 'için', 'ile', 'ise', 'kez', 'ki', 'kim', 'mı', 'mu', 'mü', 'nasıl', 'ne', 'neden', 'nerde', 'nerede', 'nereye', 'niçin', 'niye', 'o', 'sanki', 'şey', 'siz', 'şu', 'tüm', 've', 'veya', 'ya', 'yani']\n"
     ]
    }
   ],
   "source": [
    "stop_words2  = stopwords.words('turkish')\n",
    "print(stop_words2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len new_stop_words : 141\n"
     ]
    }
   ],
   "source": [
    "words_to_exclude_from_stopwords = ['not', \"n't\", 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", \"don't\", 'hadn', \n",
    "                                   \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", \n",
    "                                   'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \n",
    "                                   \"won't\", 'wouldn', \"wouldn't\"]\n",
    "\n",
    "new_stopwords = [w for w in stop_words if w not in words_to_exclude_from_stopwords]\n",
    "print('len new_stop_words :', len(new_stopwords))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['pretty', 'cool', 'good', 'quality', 'candy', 'might', 'cost', 'new', 'york', 'think', 'buy', 'said', 'cars']\n"
     ]
    }
   ],
   "source": [
    "token_without_sw = [t for t in tokens_without_punc if t not in new_stopwords] # stop_words\n",
    "print(token_without_sw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DoKQ-SVtQC7U"
   },
   "source": [
    "## Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "LGNumQbXQC7U"
   },
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "m4l4HUxCQC7U",
    "outputId": "8903fda9-66b4-46ba-8dd9-7856e70d06eb"
   },
   "outputs": [],
   "source": [
    "#nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "VWhaNmCVQC7V",
    "outputId": "5f971fb4-64c0-4fe3-e629-61a508387f6d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'driving'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'driver'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'driver'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'drive'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'drove'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "WordNetLemmatizer().lemmatize(\"driving\")\n",
    "WordNetLemmatizer().lemmatize(\"driver\")\n",
    "WordNetLemmatizer().lemmatize(\"drivers\")\n",
    "WordNetLemmatizer().lemmatize(\"drives\")\n",
    "WordNetLemmatizer().lemmatize(\"drove\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['pretty', 'cool', 'good', 'quality', 'candy', 'might', 'cost', 'new', 'york', 'think', 'buy', 'said', 'car']\n",
      "['pretty', 'cool', 'good', 'quality', 'candy', 'might', 'cost', 'new', 'york', 'think', 'buy', 'said', 'cars']\n"
     ]
    }
   ],
   "source": [
    "lem = [WordNetLemmatizer().lemmatize(t) for t in token_without_sw]\n",
    "print(lem)\n",
    "print(token_without_sw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6bgiG6R2QC7V"
   },
   "source": [
    "## Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "UcXozBsEQC7V"
   },
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer  # only supports English"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "27HrHpbzQC7V",
    "outputId": "524840db-3254-45c0-d017-34ac248d887d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'drive'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'driver'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'drive'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'drove'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'drive'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'driver'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'wa'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PorterStemmer().stem(\"driving\")\n",
    "PorterStemmer().stem(\"driver\")\n",
    "PorterStemmer().stem(\"drives\")\n",
    "PorterStemmer().stem(\"drove\")\n",
    "PorterStemmer().stem(\"Drive\")\n",
    "PorterStemmer().stem(\"drivers\")\n",
    "PorterStemmer().stem(\"was\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "2pwsWQW-QC7V"
   },
   "outputs": [],
   "source": [
    "stem = [PorterStemmer().stem(t) for t in token_without_sw]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w/o norm : ['pretty', 'cool', 'good', 'quality', 'candy', 'might', 'cost', 'new', 'york', 'think', 'buy', 'said', 'cars']\n",
      "stem     : ['pretti', 'cool', 'good', 'qualiti', 'candi', 'might', 'cost', 'new', 'york', 'think', 'buy', 'said', 'car']\n",
      "lemma    : ['pretty', 'cool', 'good', 'quality', 'candy', 'might', 'cost', 'new', 'york', 'think', 'buy', 'said', 'car']\n"
     ]
    }
   ],
   "source": [
    "print('w/o norm :', token_without_sw)\n",
    "print('stem     :', stem)\n",
    "print('lemma    :', lem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'drive'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'driver'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'drive'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'drove'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'drive'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'driver'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'was'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import SnowballStemmer  # supports multiple languages such as english, russian, french, italian...\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "\n",
    "drive_statements = ['driving', 'driver', 'drives', 'drove', 'Drive', 'drivers', 'was']\n",
    "\n",
    "for i in range(0, len(drive_statements)):\n",
    "    stemmer.stem(drive_statements[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JY3VjHdeQC7W"
   },
   "source": [
    "## Joining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "4fG6UucjQC7W"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'pretty cool good quality candy might cost new york think buy said car'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "\"this is pretty cool. a good quality candy might cost $3.88 in new york. \\n                but i don't think we will buy it. mr.biden said $1,000,000. 2 cars.\""
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\" \".join(lem)\n",
    "sample_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q-7LnA8vQC7W"
   },
   "source": [
    "### Expanding Contractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "kxGjEYJpyIqN"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I', 'will', 'not', 'be', 'there']"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_text  = word_tokenize(contractions.fix(\"I won't be there\"))\n",
    "my_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "id": "3P2YfqIBQC7W"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I will not be there'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "contractions.fix(\"I won't be there\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8Nqhpu1ZqjNG"
   },
   "source": [
    "### Part of Speech Tag\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iDQZCmChrXyO",
    "outputId": "4b5f20f0-a3f0-4899-e01e-48316a6cc928"
   },
   "outputs": [],
   "source": [
    "#nltk.download('averaged_perceptron_tagger');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "id": "2aLbZVnHqpwr"
   },
   "outputs": [],
   "source": [
    "from nltk import pos_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "id": "Ha_24Xn4qvhq"
   },
   "outputs": [],
   "source": [
    "text = \"\"\"Steven Paul Jobs was an American business magnate, industrial designer, investor, and media proprietor. \n",
    "He was the chairman, chief executive officer (CEO), and co-founder of Apple Inc.; the chairman and majority shareholder of Pixar; \n",
    "a member of The Walt Disney Company's board of directors following its acquisition of Pixar; and the founder, chairman, and CEO of NeXT.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "id": "wCR_drlTrJM2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Steven', 'Paul', 'Jobs', 'was', 'an', 'American', 'business', 'magnate', ',', 'industrial', 'designer', ',', 'investor', ',', 'and', 'media', 'proprietor', '.', 'He', 'was', 'the', 'chairman', ',', 'chief', 'executive', 'officer', '(', 'CEO', ')', ',', 'and', 'co-founder', 'of', 'Apple', 'Inc.', ';', 'the', 'chairman', 'and', 'majority', 'shareholder', 'of', 'Pixar', ';', 'a', 'member', 'of', 'The', 'Walt', 'Disney', 'Company', \"'s\", 'board', 'of', 'directors', 'following', 'its', 'acquisition', 'of', 'Pixar', ';', 'and', 'the', 'founder', ',', 'chairman', ',', 'and', 'CEO', 'of', 'NeXT', '.']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('Steven', 'NNP'),\n",
       " ('Paul', 'NNP'),\n",
       " ('Jobs', 'NNP'),\n",
       " ('was', 'VBD'),\n",
       " ('an', 'DT'),\n",
       " ('American', 'JJ'),\n",
       " ('business', 'NN'),\n",
       " ('magnate', 'NN'),\n",
       " (',', ','),\n",
       " ('industrial', 'JJ'),\n",
       " ('designer', 'NN'),\n",
       " (',', ','),\n",
       " ('investor', 'NN'),\n",
       " (',', ','),\n",
       " ('and', 'CC'),\n",
       " ('media', 'NNS'),\n",
       " ('proprietor', 'NN'),\n",
       " ('.', '.'),\n",
       " ('He', 'PRP'),\n",
       " ('was', 'VBD'),\n",
       " ('the', 'DT'),\n",
       " ('chairman', 'NN'),\n",
       " (',', ','),\n",
       " ('chief', 'JJ'),\n",
       " ('executive', 'NN'),\n",
       " ('officer', 'NN'),\n",
       " ('(', '('),\n",
       " ('CEO', 'NNP'),\n",
       " (')', ')'),\n",
       " (',', ','),\n",
       " ('and', 'CC'),\n",
       " ('co-founder', 'NN'),\n",
       " ('of', 'IN'),\n",
       " ('Apple', 'NNP'),\n",
       " ('Inc.', 'NNP'),\n",
       " (';', ':'),\n",
       " ('the', 'DT'),\n",
       " ('chairman', 'NN'),\n",
       " ('and', 'CC'),\n",
       " ('majority', 'NN'),\n",
       " ('shareholder', 'NN'),\n",
       " ('of', 'IN'),\n",
       " ('Pixar', 'NNP'),\n",
       " (';', ':'),\n",
       " ('a', 'DT'),\n",
       " ('member', 'NN'),\n",
       " ('of', 'IN'),\n",
       " ('The', 'DT'),\n",
       " ('Walt', 'NNP'),\n",
       " ('Disney', 'NNP'),\n",
       " ('Company', 'NNP'),\n",
       " (\"'s\", 'POS'),\n",
       " ('board', 'NN'),\n",
       " ('of', 'IN'),\n",
       " ('directors', 'NNS'),\n",
       " ('following', 'VBG'),\n",
       " ('its', 'PRP$'),\n",
       " ('acquisition', 'NN'),\n",
       " ('of', 'IN'),\n",
       " ('Pixar', 'NNP'),\n",
       " (';', ':'),\n",
       " ('and', 'CC'),\n",
       " ('the', 'DT'),\n",
       " ('founder', 'NN'),\n",
       " (',', ','),\n",
       " ('chairman', 'NN'),\n",
       " (',', ','),\n",
       " ('and', 'CC'),\n",
       " ('CEO', 'NNP'),\n",
       " ('of', 'IN'),\n",
       " ('NeXT', 'NNP'),\n",
       " ('.', '.')]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = word_tokenize(text)\n",
    "print(tokens)\n",
    "pos = pos_tag(tokens)\n",
    "pos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ko_b0xHlqXk0"
   },
   "source": [
    "### NER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DPLpyNNetTxA",
    "outputId": "b0c94a1a-faee-487d-ff39-43d9f43f199a"
   },
   "outputs": [],
   "source": [
    "#nltk.download('maxent_ne_chunker')\n",
    "#nltk.download('words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "id": "0JY5oaVPqc-9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steven Paul Jobs was an American business magnate, industrial designer, investor, and media proprietor. \n",
      "He was the chairman, chief executive officer (CEO), and co-founder of Apple Inc.; the chairman and majority shareholder of Pixar; \n",
      "a member of The Walt Disney Company's board of directors following its acquisition of Pixar; and the founder, chairman, and CEO of NeXT. \n",
      "\n",
      "PERSON Steven\n",
      "PERSON Paul Jobs\n",
      "GPE American\n",
      "ORGANIZATION CEO\n",
      "ORGANIZATION Apple Inc.\n",
      "GPE Pixar\n",
      "ORGANIZATION Walt Disney Company\n",
      "GPE Pixar\n",
      "ORGANIZATION CEO\n",
      "ORGANIZATION NeXT\n"
     ]
    }
   ],
   "source": [
    "from nltk import ne_chunk\n",
    "print(text, '\\n')\n",
    "for chunk in nltk.ne_chunk(pos):\n",
    "      if hasattr(chunk, 'label'):\n",
    "        print(chunk.label(), ' '.join(c[0] for c in chunk))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Challenges!\n",
    "# Apple is shaping the industry. vs Apple means more than fruit\n",
    "# Costa is a good guy. vs Costa Rica is a beautiful country."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fhd7AxIvQC7X"
   },
   "source": [
    "## Cleaning Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "id": "o6OrzWV8nIdZ"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I', \"n't\", 'want', 'fly', 'company', '.']"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "['I', 'dont', 'want', 'fly', 'company']"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = \"I don't want to fly with your company.\" # vs \n",
    "[token for token in word_tokenize(a) if token not in stop_words] \n",
    "b = \"I don't want to fly with your company\".replace(\"'\", '') \n",
    "[token for token in word_tokenize(b) if token not in stop_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "id": "kQcS0m0_mdTt"
   },
   "outputs": [],
   "source": [
    "def cleaning(data: str) -> str:\n",
    "    \n",
    "    #1. Contractions Expension & Tokenize\n",
    "    text_tokens = word_tokenize(contractions.fix(data.lower())) \n",
    "    # text_tokens = word_tokenize(data.replace(\"'\", '').lower())\n",
    "    \n",
    "    #2. Remove Puncs & numbers\n",
    "    tokens_without_punc = [w for w in text_tokens if w.isalpha()]\n",
    "    \n",
    "    #3. Removing Stopwords\n",
    "    tokens_without_sw = [t for t in tokens_without_punc if t not in new_stopwords]\n",
    "    \n",
    "    #4. lemma\n",
    "    text_cleaned = [WordNetLemmatizer().lemmatize(t) for t in tokens_without_sw]\n",
    "    \n",
    "    #joining\n",
    "    return \" \".join(text_cleaned)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8fadu17EQC7X"
   },
   "source": [
    "## CountVectorization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e18FSxq4TuQJ"
   },
   "source": [
    "#### Data\n",
    "🔑 Source: https://www.kaggle.com/crowdflower/twitter-airline-sentiment?select=Tweets.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Sv7bi6bHQvDe",
    "outputId": "fa5fc0f8-f057-4784-cda6-25c089b27448"
   },
   "outputs": [],
   "source": [
    "# For Colab\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "id": "eqImA_4OQC7X"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "#df = pd.read_csv('/content/gdrive/MyDrive/Colab Notebooks/Clarusway_NLP/Clarusway/clarusway-ds-students-7-21-main/3- Classes_Labs/NLP/NLP-1/airline_tweets.csv')\n",
    "df = pd.read_csv(\"airline_tweets.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "id": "0nQnCr5TQC7Y"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>airline_sentiment</th>\n",
       "      <th>airline_sentiment_confidence</th>\n",
       "      <th>negativereason</th>\n",
       "      <th>negativereason_confidence</th>\n",
       "      <th>airline</th>\n",
       "      <th>airline_sentiment_gold</th>\n",
       "      <th>name</th>\n",
       "      <th>negativereason_gold</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>text</th>\n",
       "      <th>tweet_coord</th>\n",
       "      <th>tweet_created</th>\n",
       "      <th>tweet_location</th>\n",
       "      <th>user_timezone</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>570306133677760513</td>\n",
       "      <td>neutral</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>cairdin</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica What @dhepburn said.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:35:52 -0800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Eastern Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>570301130888122368</td>\n",
       "      <td>positive</td>\n",
       "      <td>0.3486</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>jnardino</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica plus you've added commercials to the experience... tacky.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:15:59 -0800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Pacific Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>570301083672813571</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0.6837</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>yvonnalynn</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica I didn't today... Must mean I need to take another trip!</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:15:48 -0800</td>\n",
       "      <td>Lets Play</td>\n",
       "      <td>Central Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>570301031407624196</td>\n",
       "      <td>negative</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Bad Flight</td>\n",
       "      <td>0.7033</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>jnardino</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica it's really aggressive to blast obnoxious \"entertainment\" in your guests' faces &amp;amp; they have little recourse</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:15:36 -0800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Pacific Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>570300817074462722</td>\n",
       "      <td>negative</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Can't Tell</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>jnardino</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica and it's a really big bad thing about it</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:14:45 -0800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Pacific Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             tweet_id airline_sentiment  airline_sentiment_confidence  \\\n",
       "0  570306133677760513           neutral                        1.0000   \n",
       "1  570301130888122368          positive                        0.3486   \n",
       "2  570301083672813571           neutral                        0.6837   \n",
       "3  570301031407624196          negative                        1.0000   \n",
       "4  570300817074462722          negative                        1.0000   \n",
       "\n",
       "  negativereason  negativereason_confidence         airline  \\\n",
       "0            NaN                        NaN  Virgin America   \n",
       "1            NaN                     0.0000  Virgin America   \n",
       "2            NaN                        NaN  Virgin America   \n",
       "3     Bad Flight                     0.7033  Virgin America   \n",
       "4     Can't Tell                     1.0000  Virgin America   \n",
       "\n",
       "  airline_sentiment_gold        name negativereason_gold  retweet_count  \\\n",
       "0                    NaN     cairdin                 NaN              0   \n",
       "1                    NaN    jnardino                 NaN              0   \n",
       "2                    NaN  yvonnalynn                 NaN              0   \n",
       "3                    NaN    jnardino                 NaN              0   \n",
       "4                    NaN    jnardino                 NaN              0   \n",
       "\n",
       "                                                                                                                             text  \\\n",
       "0                                                                                             @VirginAmerica What @dhepburn said.   \n",
       "1                                                        @VirginAmerica plus you've added commercials to the experience... tacky.   \n",
       "2                                                         @VirginAmerica I didn't today... Must mean I need to take another trip!   \n",
       "3  @VirginAmerica it's really aggressive to blast obnoxious \"entertainment\" in your guests' faces &amp; they have little recourse   \n",
       "4                                                                         @VirginAmerica and it's a really big bad thing about it   \n",
       "\n",
       "  tweet_coord              tweet_created tweet_location  \\\n",
       "0         NaN  2015-02-24 11:35:52 -0800            NaN   \n",
       "1         NaN  2015-02-24 11:15:59 -0800            NaN   \n",
       "2         NaN  2015-02-24 11:15:48 -0800      Lets Play   \n",
       "3         NaN  2015-02-24 11:15:36 -0800            NaN   \n",
       "4         NaN  2015-02-24 11:14:45 -0800            NaN   \n",
       "\n",
       "                user_timezone  \n",
       "0  Eastern Time (US & Canada)  \n",
       "1  Pacific Time (US & Canada)  \n",
       "2  Central Time (US & Canada)  \n",
       "3  Pacific Time (US & Canada)  \n",
       "4  Pacific Time (US & Canada)  "
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "id": "mOtx_XuLQC7Y"
   },
   "outputs": [],
   "source": [
    "data = [text for text in df['text'][:5]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "id": "2fw_lCtJQC7Y"
   },
   "outputs": [],
   "source": [
    "# Tweet Tokenizer\n",
    "#for i in range(0, len(data)):\n",
    "#    data[i] = tk.tokenize(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['@VirginAmerica What @dhepburn said.',\n",
       " \"@VirginAmerica plus you've added commercials to the experience... tacky.\",\n",
       " \"@VirginAmerica I didn't today... Must mean I need to take another trip!\",\n",
       " '@VirginAmerica it\\'s really aggressive to blast obnoxious \"entertainment\" in your guests\\' faces &amp; they have little recourse',\n",
       " \"@VirginAmerica and it's a really big bad thing about it\"]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's check the data\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "id": "Y-jRQzliQC7Z"
   },
   "outputs": [],
   "source": [
    "vec = CountVectorizer()\n",
    "#vec = CountVectorizer(stop_words='english')\n",
    "docs = vec.fit_transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['about', 'added', 'aggressive', 'amp', 'and', 'another', 'bad',\n",
       "       'big', 'blast', 'commercials', 'dhepburn', 'didn', 'entertainment',\n",
       "       'experience', 'faces', 'guests', 'have', 'in', 'it', 'little',\n",
       "       'mean', 'must', 'need', 'obnoxious', 'plus', 'really', 'recourse',\n",
       "       'said', 'tacky', 'take', 'the', 'they', 'thing', 'to', 'today',\n",
       "       'trip', 've', 'virginamerica', 'what', 'you', 'your'], dtype=object)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features = vec.get_feature_names_out()  # get_feature_names()\n",
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "id": "zwZbgya9QC7Z"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<5x41 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 49 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "41"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features.__len__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0],\n",
       "       [0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0],\n",
       "       [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1,\n",
       "        1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0],\n",
       "       [0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0,\n",
       "        0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1],\n",
       "       [1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0,\n",
       "        0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0]])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('about', 1),\n",
       " ('added', 1),\n",
       " ('aggressive', 1),\n",
       " ('amp', 1),\n",
       " ('and', 1),\n",
       " ('another', 1),\n",
       " ('bad', 1),\n",
       " ('big', 1),\n",
       " ('blast', 1),\n",
       " ('commercials', 1),\n",
       " ('dhepburn', 1),\n",
       " ('didn', 1),\n",
       " ('entertainment', 1),\n",
       " ('experience', 1),\n",
       " ('faces', 1),\n",
       " ('guests', 1),\n",
       " ('have', 1),\n",
       " ('in', 1),\n",
       " ('it', 3),\n",
       " ('little', 1),\n",
       " ('mean', 1),\n",
       " ('must', 1),\n",
       " ('need', 1),\n",
       " ('obnoxious', 1),\n",
       " ('plus', 1),\n",
       " ('really', 2),\n",
       " ('recourse', 1),\n",
       " ('said', 1),\n",
       " ('tacky', 1),\n",
       " ('take', 1),\n",
       " ('the', 1),\n",
       " ('they', 1),\n",
       " ('thing', 1),\n",
       " ('to', 3),\n",
       " ('today', 1),\n",
       " ('trip', 1),\n",
       " ('ve', 1),\n",
       " ('virginamerica', 5),\n",
       " ('what', 1),\n",
       " ('you', 1),\n",
       " ('your', 1)]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(zip(features, docs.toarray().sum(axis=0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>about</th>\n",
       "      <th>added</th>\n",
       "      <th>aggressive</th>\n",
       "      <th>amp</th>\n",
       "      <th>and</th>\n",
       "      <th>another</th>\n",
       "      <th>bad</th>\n",
       "      <th>big</th>\n",
       "      <th>blast</th>\n",
       "      <th>commercials</th>\n",
       "      <th>...</th>\n",
       "      <th>they</th>\n",
       "      <th>thing</th>\n",
       "      <th>to</th>\n",
       "      <th>today</th>\n",
       "      <th>trip</th>\n",
       "      <th>ve</th>\n",
       "      <th>virginamerica</th>\n",
       "      <th>what</th>\n",
       "      <th>you</th>\n",
       "      <th>your</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 41 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   about  added  aggressive  amp  and  another  bad  big  blast  commercials  \\\n",
       "0      0      0           0    0    0        0    0    0      0            0   \n",
       "1      0      1           0    0    0        0    0    0      0            1   \n",
       "2      0      0           0    0    0        1    0    0      0            0   \n",
       "3      0      0           1    1    0        0    0    0      1            0   \n",
       "4      1      0           0    0    1        0    1    1      0            0   \n",
       "\n",
       "   ...  they  thing  to  today  trip  ve  virginamerica  what  you  your  \n",
       "0  ...     0      0   0      0     0   0              1     1    0     0  \n",
       "1  ...     0      0   1      0     0   1              1     0    1     0  \n",
       "2  ...     0      0   1      1     1   0              1     0    0     0  \n",
       "3  ...     1      0   1      0     0   0              1     0    0     1  \n",
       "4  ...     0      1   0      0     0   0              1     0    0     0  \n",
       "\n",
       "[5 rows x 41 columns]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_vectorized = pd.DataFrame(docs.toarray(), columns=features)\n",
    "df_vectorized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"@VirginAmerica seriously would pay $30 a flight for seats that didn't have this playing.\\nit's really the only bad thing about flying VA\",\n",
       " '@VirginAmerica yes, nearly every time I fly VX this “ear worm” won’t go away :)']"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_data = [text for text in df['text'][5:7]]\n",
    "new_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<2x41 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 10 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_docs = vec.transform(new_data)\n",
    "new_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0,\n",
       "        0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_docs.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0\n",
      " 1 1 0 0]\n",
      "[1 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 1 0 1 0 0 0 0 0 0 1 0 0 0 0 1 0 1 0 0 0 0\n",
      " 1 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "print(docs.toarray()[0])\n",
    "print(new_docs.toarray()[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BfCSqNgbQC7Z"
   },
   "source": [
    "### Hashing Vectorizer\n",
    "Memory efficient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "id": "2eGZ03JjERKc"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import HashingVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "id": "xymK6fJADf5M"
   },
   "outputs": [],
   "source": [
    "# Check it after the class!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Important elements of CountVectorization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-> n-gram\n",
    "-> min_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### n-gram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Without *n-*grams, the relative proximity of words is ignored. \n",
    "> e.g, credit score —> they are meaningful together."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The downside to using n-grams is that it increases memory consumption and training time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### min_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> `min_df`, which ignores words that appear fewer than the specified number of times\n",
    "> \n",
    "> - integer or float are valid\n",
    "> - filtering and reduces the memory usage and training time\n",
    "> - also max_df can be used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec_2 = CountVectorizer(stop_words='english', ngram_range=(1,2), min_df = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_2 = vec_2.fit_transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['really', 'virginamerica', 'virginamerica really'], dtype=object)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vec_2.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2KVi7vfzQC7c"
   },
   "source": [
    "## TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0Y2lePoRQC7c"
   },
   "source": [
    "🔑 : sklearn TD-IDF\n",
    "https://towardsdatascience.com/how-sklearns-tf-idf-is-different-from-the-standard-tf-idf-275fa582e73d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "id": "Oo2j3kjzQC7c"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = [\"Similar problems similar solutions\",\n",
    "              \"Why do you ignore these problems\",\n",
    "              \"Statistical challenges cause different problems\",\n",
    "              \"Sometimes solutions are the simplest and easiest ones\"\n",
    "             ]\n",
    "\n",
    "test_data = [\"Life is beautiful and peaceful\",\n",
    "             \"NLP problems mostly require simple solutions\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "id": "euxCrAL-QC7c"
   },
   "outputs": [],
   "source": [
    "tf_idf_vectorizer = TfidfVectorizer()\n",
    "train_tf_idf = tf_idf_vectorizer.fit_transform(train_data)\n",
    "test_tf_idf = tf_idf_vectorizer.transform(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "byiZxyK5QC7c",
    "outputId": "0d2efe9d-0fa2-4dc0-e53b-088cc0303106"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['and', 'are', 'cause', 'challenges', 'different', 'do', 'easiest',\n",
       "       'ignore', 'ones', 'problems', 'similar', 'simplest', 'solutions',\n",
       "       'sometimes', 'statistical', 'the', 'these', 'why', 'you'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_idf_features = tf_idf_vectorizer.get_feature_names_out()\n",
    "tf_idf_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9FfCRDWxQC7d",
    "outputId": "11b45064-9719-44ce-ae21-fcfac665a423"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.28462634,\n",
       "        0.89184431, 0.        , 0.35157015, 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.43003652, 0.        , 0.43003652, 0.        , 0.27448674,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.43003652, 0.43003652, 0.43003652],\n",
       "       [0.        , 0.        , 0.47633035, 0.47633035, 0.47633035,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.30403549,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.47633035,\n",
       "        0.        , 0.        , 0.        , 0.        ],\n",
       "       [0.36222393, 0.36222393, 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.36222393, 0.        , 0.36222393, 0.        ,\n",
       "        0.        , 0.36222393, 0.2855815 , 0.36222393, 0.        ,\n",
       "        0.36222393, 0.        , 0.        , 0.        ]])"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_tf_idf.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 456
    },
    "id": "SJY0L5kYQC7d",
    "outputId": "935d2fdb-0135-4eed-fd03-a1685e918e46"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>and</th>\n",
       "      <th>are</th>\n",
       "      <th>cause</th>\n",
       "      <th>challenges</th>\n",
       "      <th>different</th>\n",
       "      <th>do</th>\n",
       "      <th>easiest</th>\n",
       "      <th>ignore</th>\n",
       "      <th>ones</th>\n",
       "      <th>problems</th>\n",
       "      <th>similar</th>\n",
       "      <th>simplest</th>\n",
       "      <th>solutions</th>\n",
       "      <th>sometimes</th>\n",
       "      <th>statistical</th>\n",
       "      <th>the</th>\n",
       "      <th>these</th>\n",
       "      <th>why</th>\n",
       "      <th>you</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.284626</td>\n",
       "      <td>0.891844</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.351570</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.430037</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.430037</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.274487</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.430037</td>\n",
       "      <td>0.430037</td>\n",
       "      <td>0.430037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.47633</td>\n",
       "      <td>0.47633</td>\n",
       "      <td>0.47633</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.304035</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.47633</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.362224</td>\n",
       "      <td>0.362224</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.362224</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.362224</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.362224</td>\n",
       "      <td>0.285582</td>\n",
       "      <td>0.362224</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.362224</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        and       are    cause  challenges  different        do   easiest  \\\n",
       "0  0.000000  0.000000  0.00000     0.00000    0.00000  0.000000  0.000000   \n",
       "1  0.000000  0.000000  0.00000     0.00000    0.00000  0.430037  0.000000   \n",
       "2  0.000000  0.000000  0.47633     0.47633    0.47633  0.000000  0.000000   \n",
       "3  0.362224  0.362224  0.00000     0.00000    0.00000  0.000000  0.362224   \n",
       "\n",
       "     ignore      ones  problems   similar  simplest  solutions  sometimes  \\\n",
       "0  0.000000  0.000000  0.284626  0.891844  0.000000   0.351570   0.000000   \n",
       "1  0.430037  0.000000  0.274487  0.000000  0.000000   0.000000   0.000000   \n",
       "2  0.000000  0.000000  0.304035  0.000000  0.000000   0.000000   0.000000   \n",
       "3  0.000000  0.362224  0.000000  0.000000  0.362224   0.285582   0.362224   \n",
       "\n",
       "   statistical       the     these       why       you  \n",
       "0      0.00000  0.000000  0.000000  0.000000  0.000000  \n",
       "1      0.00000  0.000000  0.430037  0.430037  0.430037  \n",
       "2      0.47633  0.000000  0.000000  0.000000  0.000000  \n",
       "3      0.00000  0.362224  0.000000  0.000000  0.000000  "
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(train_tf_idf.toarray(), columns = tf_idf_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "id": "QUfGIWcEKCe4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Similar problems similar solutions',\n",
       " 'Why do you ignore these problems',\n",
       " 'Statistical challenges cause different problems',\n",
       " 'Sometimes solutions are the simplest and easiest ones']"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_data = train_data.copy()\n",
    "for i in range(0, len(train_data)):\n",
    "    clean_data[i] = cleaning(train_data[i])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['similar problem similar solution',\n",
       " 'ignore problem',\n",
       " 'statistical challenge different problem',\n",
       " 'sometimes solution simplest easiest one']"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_test_data = test_data.copy()\n",
    "for i in range(0, len(test_data)):\n",
    "    clean_test_data[i] = cleaning(test_data[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "cl_tf_idf_vectorizer = TfidfVectorizer()\n",
    "cl_train_tf_idf = cl_tf_idf_vectorizer.fit_transform(clean_data)\n",
    "cl_test_tf_idf = cl_tf_idf_vectorizer.transform(clean_test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['challenge', 'different', 'easiest', 'ignore', 'one', 'problem',\n",
       "       'similar', 'simplest', 'solution', 'sometimes', 'statistical'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cl_features = cl_tf_idf_vectorizer.get_feature_names_out()\n",
    "cl_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['and', 'are', 'cause', 'challenges', 'different', 'do', 'easiest',\n",
       "       'ignore', 'ones', 'problems', 'similar', 'simplest', 'solutions',\n",
       "       'sometimes', 'statistical', 'the', 'these', 'why', 'you'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_idf_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19\n",
      "11\n"
     ]
    }
   ],
   "source": [
    "print(tf_idf_features.__len__())\n",
    "print(cl_features.__len__())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.62922751, 0.        , 0.        , 0.77722116, 0.        ,\n",
       "        0.        ]])"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cl_test_tf_idf.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "NLP-1 (Data Cleaning-Vectorization)(ENG)-23 June_FC-S.ipynb",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
